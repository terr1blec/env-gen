{
  "labels": {
    "analysis": "The MCP Server leverages Google's Gemini model to provide powerful natural language processing capabilities through a standardized tool-based interface. Its core functionalities include text generation (both streaming and non-streaming), stateful chat management, file operations (uploading, listing, retrieving, deleting), and cache management for optimizing API usage. The server is designed to facilitate seamless integration with LLM applications, allowing developers to handle complex AI tasks efficiently.",
    "reasoning": "The primary functionality of this server revolves around AI-powered text generation and stateful conversational interactions, which aligns most closely with the \"AI/ML Tools\" category. Secondary labels for \"Memory Management\" and \"File Management\" are included due to its capabilities for handling file operations and cache management. The custom label \"LLM Integration\" emphasizes its specialized role in facilitating interactions with large language models.",
    "primary_label": "AI/ML Tools",
    "secondary_labels": [
      "Memory Management",
      "File Management"
    ],
    "custom_label": "LLM Integration",
    "is_connected": false,
    "is_remote_tool_valid": false,
    "featured_server": false
  },
  "metadata": {
    "server_id": 97,
    "server_name": "Gemini Server",
    "rank_by_usage": 98,
    "usage_count": "721",
    "original_file": "../crawler/smithery/@bsmi021_mcp-gemini-server.json",
    "mode": "smithery",
    "timestamp": 1751941824,
    "remote_server_response": {
      "url": "https://server.smithery.ai/@bsmi021/mcp-gemini-server/mcp?api_key=8675feae-43b6-4170-beb5-d8fa5a938222&profile=monetary-anteater-CCaAaT",
      "is_success": false,
      "error": "unhandled errors in a TaskGroup (1 sub-exception)",
      "tools": [],
      "tool_count": 0,
      "tool_names": []
    },
    "server_info_crawled": {
      "id": 97,
      "name": "Gemini Server",
      "author": "@bsmi021/mcp-gemini-server",
      "overview": "Leverage Google's Gemini model capabilities seamlessly with your LLM applications. Generate text, manage stateful chats, and handle files efficiently through a standardized tool-based interface. Simplify your integration with powerful features designed for optimal performance and ease of use.",
      "repository_url": "https://github.com/bsmi021/mcp-gemini-server",
      "homepage": "https://smithery.ai/server/@bsmi021/mcp-gemini-server",
      "remote_or_local": "Remote",
      "license": "Smithery",
      "usage_count": "721",
      "success_rate": "98.68%",
      "tags": [
        "search",
        "web",
        "api",
        "mcp"
      ],
      "categories": [
        "search",
        "api"
      ],
      "file_path": "../crawler/smithery/@bsmi021_mcp-gemini-server.json",
      "tools_count": 16,
      "tools": [
        {
          "name": "exampleTool",
          "description": "Deploy Server Gemini Server Claim Server @bsmi021/mcp-gemini-server Try in Playground exampleTool",
          "parameters": [
            {
              "name": "exampleTool",
              "required": false,
              "type": "string"
            }
          ]
        },
        {
          "name": "gemini_generateContent",
          "description": "Generates non-streaming text content using a specified Google Gemini model. This tool takes a text prompt and returns the complete generated response from the model. It's suitable for single-turn generation tasks where the full response is needed at once. Optional parameters allow control over generation (temperature, max tokens, etc.) and safety settings.",
          "parameters": []
        },
        {
          "name": "gemini_generateContentStream",
          "description": "Generates text content as a stream using a specified Google Gemini model. This tool takes a text prompt and streams back chunks of the generated response as they become available. It's suitable for interactive use cases or handling long responses. Optional parameters allow control over generation and safety settings.",
          "parameters": []
        },
        {
          "name": "gemini_functionCall",
          "description": "Generates content using a specified Google Gemini model, enabling the model to request execution of predefined functions. This tool accepts function declarations and returns either the standard text response OR the details of a function call requested by the model. NOTE: This tool only returns the *request* for a function call; it does not execute the function itself.",
          "parameters": []
        },
        {
          "name": "gemini_startChat",
          "description": "Initiates a new stateful chat session with a specified Gemini model. Returns a unique sessionId to be used in subsequent chat messages. Optionally accepts initial conversation history and session-wide generation/safety configurations.",
          "parameters": [
            {
              "name": "gemini_startChat",
              "required": false,
              "type": "string"
            }
          ]
        },
        {
          "name": "gemini_sendMessage",
          "description": "Sends a message to an existing Gemini chat session, identified by its sessionId. Returns the model's response, which might include text or a function call request.",
          "parameters": [
            {
              "name": "gemini_sendMessage",
              "required": false,
              "type": "string"
            }
          ]
        },
        {
          "name": "gemini_sendFunctionResult",
          "description": "Sends the result(s) of function execution(s) back to an existing Gemini chat session, identified by its sessionId. Returns the model's subsequent response.",
          "parameters": [
            {
              "name": "gemini_sendFunctionResult",
              "required": false,
              "type": "string"
            }
          ]
        },
        {
          "name": "gemini_uploadFile",
          "description": "Uploads a file (specified by a local path) to be used with the Gemini API. NOTE: This API is not supported on Vertex AI clients. It only works with Google AI Studio API keys. Returns metadata about the uploaded file, including its unique name and URI.",
          "parameters": []
        },
        {
          "name": "gemini_listFiles",
          "description": "Lists files previously uploaded to the Gemini API. Supports pagination to handle large numbers of files. NOTE: This API is not supported on Vertex AI clients. It only works with Google AI Studio API keys. Returns a list of file metadata objects and potentially a token for the next page.",
          "parameters": []
        },
        {
          "name": "gemini_getFile",
          "description": "Retrieves metadata for a specific file previously uploaded to the Gemini API. NOTE: This API is not supported on Vertex AI clients. It only works with Google AI Studio API keys. Requires the unique file name (e.g., 'files/abc123xyz').",
          "parameters": []
        },
        {
          "name": "gemini_deleteFile",
          "description": "Deletes a specific file previously uploaded to the Gemini API. NOTE: This API is not supported on Vertex AI clients. It only works with Google AI Studio API keys. Requires the unique file name (e.g., 'files/abc123xyz'). Returns a success confirmation.",
          "parameters": []
        },
        {
          "name": "gemini_createCache",
          "description": "Creates a cached content resource for a compatible Gemini model. Caching can reduce latency and costs for prompts that are reused often. NOTE: Caching is only supported for specific models (e.g., gemini-1.5-flash, gemini-1.5-pro). Returns metadata about the created cache.",
          "parameters": []
        },
        {
          "name": "gemini_listCaches",
          "description": "Lists cached content resources available for the project. Supports pagination. Returns a list of cache metadata objects and potentially a token for the next page.",
          "parameters": [
            {
              "name": "Supports",
              "required": false,
              "type": "string"
            }
          ]
        },
        {
          "name": "gemini_getCache",
          "description": "Retrieves metadata for a specific cached content resource. Requires the unique cache name (e.g., 'cachedContents/abc123xyz').",
          "parameters": []
        },
        {
          "name": "gemini_updateCache",
          "description": "Updates metadata (TTL and/or displayName) for a specific cached content resource. Requires the unique cache name (e.g., 'cachedContents/abc123xyz'). Returns the updated cache metadata.",
          "parameters": []
        },
        {
          "name": "gemini_deleteCache",
          "description": "Deletes a specific cached content resource. Requires the unique cache name (e.g., 'cachedContents/abc123xyz'). Returns a success confirmation.",
          "parameters": []
        }
      ],
      "python_sdk": "import mcp\nfrom mcp.client.streamable_http import streamablehttp_client\nimport json\nimport base64\n\nsmithery_api_key = \"\"\nurl = f\"https://server.smithery.ai/@bsmi021/mcp-gemini-server/mcp?api_key={smithery_api_key}\"\n\nasync def main():\n    # Connect to the server using HTTP client\n    async with streamablehttp_client(url) as (read_stream, write_stream, _):\n        async with mcp.ClientSession(read_stream, write_stream) as session:\n            # Initialize the connection\n            await session.initialize()\n            # List available tools\n            tools_result = await session.list_tools()\n            print(f\"Available tools: {', '.join([t.name for t in tools_result.tools])}\")\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())",
      "configuration_schema": "",
      "smithery_configuration_requirements": [
        {
          "name": "GOOGLE_GEMINI_API_KEY",
          "required": true,
          "description": "Your API key from Google AI Studio."
        }
      ],
      "python_sdk_config": "",
      "python_sdk_url": "https://server.smithery.ai/@bsmi021/mcp-gemini-server/mcp?api_key={smithery_api_key}"
    },
    "source_filename": "cf_0098.@bsmi021_mcp-gemini-server_prepared.json",
    "processed_timestamp": 1753731940,
    "processing_mode": "smithery",
    "rank": 95
  }
}