{
  "labels": {
    "analysis": "The MCP Server is primarily focused on automated web browsing and scraping tasks with AI-powered capabilities. It excels in searching the web, navigating websites, and extracting structured content from sources like GitHub, Stack Overflow, and other documentation sites. Additionally, it offers file management and directory navigation functionalities, allowing users to list, preview, and manipulate files. This combination makes it particularly valuable for research, data collection, and information processing tasks.",
    "reasoning": "The primary label \"Browser Automation\" is chosen because the server's core functionality revolves around automated browsing, web scraping, and intelligent content extraction. Secondary labels like \"Web Search & Research\" and \"File Management\" are included because these tools significantly enhance the server's ability to gather and process information, and for handling file operations often associated with research and automation tasks. The custom label \"Content Extraction Agent\" captures the server's unique specialty in retrieving and processing web content for further use.",
    "primary_label": "Browser Automation",
    "secondary_labels": [
      "Web Search & Research",
      "File Management"
    ],
    "custom_label": "Content Extraction Agent",
    "is_connected": false,
    "is_remote_tool_valid": false,
    "featured_server": false
  },
  "metadata": {
    "server_id": 1133,
    "server_name": "Browser Automation Agent",
    "rank_by_usage": 1134,
    "usage_count": "1",
    "original_file": "../crawler/smithery/@Raghu6798_browser_scrape_mcp.json",
    "mode": "smithery",
    "timestamp": 1751941824,
    "remote_server_response": {
      "url": "https://server.smithery.ai/@Raghu6798/browser_scrape_mcp/mcp?api_key=8675feae-43b6-4170-beb5-d8fa5a938222&profile=monetary-anteater-CCaAaT",
      "is_success": false,
      "error": "unhandled errors in a TaskGroup (1 sub-exception)",
      "tools": [],
      "tool_count": 0,
      "tool_names": []
    },
    "server_info_crawled": {
      "id": 1133,
      "name": "Browser Automation Agent",
      "author": "@Raghu6798/browser_scrape_mcp",
      "overview": "Automate web browsing and scraping tasks with intelligent, AI-powered capabilities. Search Google, navigate websites, and extract tailored content from GitHub, Stack Overflow, documentation, and generic sites. Save screenshots and text content automatically for efficient data collection and processing.",
      "repository_url": "https://github.com/Raghu6798/Browser_scrape_mcp",
      "homepage": "https://smithery.ai/server/@Raghu6798/browser_scrape_mcp",
      "remote_or_local": "Remote",
      "license": "Smithery",
      "usage_count": "1",
      "success_rate": "Not available",
      "tags": [
        "search",
        "web",
        "api",
        "mcp"
      ],
      "categories": [
        "search",
        "api"
      ],
      "file_path": "../crawler/smithery/@Raghu6798_browser_scrape_mcp.json",
      "tools_count": 11,
      "tools": [
        {
          "name": "search_and_scrape",
          "description": "Deploy Server Browser Automation Agent @Raghu6798/browser_scrape_mcp Try in Playground search_and_scrape",
          "parameters": []
        },
        {
          "name": "list_directory",
          "description": "List contents of a directory. This tool lists all files and directories in the specified path. If no path is provided, it lists the current directory. Args: path (str, optional): The directory path to list. Defaults to current directory (\".\"). Returns: list: A list of dictionaries containing information about each item: - name: The name of the file/directory - type: Either \"file\" or \"directory\" - size: File size in bytes (for files only) - modified: Last modification timestamp Example: >>> contents = list_directory(\"/path/to/directory\") >>> print(contents)",
          "parameters": []
        },
        {
          "name": "get_current_directory",
          "description": "Get the current working directory. Returns: str: The absolute path of the current working directory. Example: >>> current_dir = get_current_directory() >>> print(current_dir)",
          "parameters": [
            {
              "name": "current_dir",
              "required": false,
              "type": "string"
            }
          ]
        },
        {
          "name": "change_directory",
          "description": "Change the current working directory. Args: path (str): The directory path to change to. Returns: str: The new current working directory path. Raises: Exception: If the directory doesn't exist or is not accessible. Example: >>> new_dir = change_directory(\"/path/to/directory\") >>> print(new_dir)",
          "parameters": []
        },
        {
          "name": "file_info",
          "description": "Get detailed information about a file or directory. Args: path (str): The path to the file or directory. Can be obtained from list_all_files()[\"files\"][i][\"path\"]. Returns: dict: A dictionary containing: - exists: Whether the path exists - type: \"file\" or \"directory\" - size: Size in bytes (for files) - created: Creation timestamp - modified: Last modification timestamp - accessed: Last access timestamp - absolute_path: Full absolute path Example: >>> # Get all files first >>> all_files = list_all_files() >>> # Get info for first file >>> info = file_info(all_files[\"files\"][0][\"path\"]) >>> print(info)",
          "parameters": []
        },
        {
          "name": "create_directory",
          "description": "Create a new directory. Args: path (str): The path where the directory should be created. Returns: dict: A dictionary containing: - success: Boolean indicating if creation was successful - path: The created directory path - error: Error message if creation failed Example: >>> result = create_directory(\"/path/to/new/directory\") >>> print(result)",
          "parameters": []
        },
        {
          "name": "scrape_content",
          "description": "Scrape content from a given URL and return it in markdown format. This tool uses Firecrawl to extract content from a webpage and convert it to markdown format. It's designed to handle various types of web content and convert them into a consistent markdown representation. Args: url (str): The URL of the webpage to scrape. Must be a valid HTTP/HTTPS URL. Returns: str: The scraped content in markdown format. Example: >>> content = scrape_content(\"https://example.com\") >>> print(content) Raises: Exception: If the URL is invalid or if the scraping process fails.",
          "parameters": []
        },
        {
          "name": "read_file_content",
          "description": "Read and display the contents of a file with proper formatting. This tool reads a file and returns its contents with metadata. For text files, it can optionally return specific line ranges. For markdown files, it includes rendered content. Args: file_path (str): The path to the file to read. Can be obtained from list_all_files()[\"files\"][i][\"path\"]. start_line (int, optional): Starting line number to read. Defaults to 1. end_line (int, optional): Ending line number to read. If None, reads entire file. Returns: dict: A dictionary containing: - content: The file contents - rendered_content: Rendered markdown if applicable - metadata: File information (size, type, etc.) - error: Error message if reading fails Example: >>> # Get all files first >>> all_files = list_all_files() >>> # Read content of first file >>> result = read_file_content(all_files[\"files\"][0][\"path\"]) >>> print(result[\"content\"])",
          "parameters": []
        },
        {
          "name": "preview_file",
          "description": "Preview the beginning of a file. This tool reads and displays the first few lines of a file, useful for quick file content inspection. Args: file_path (str): The path to the file to preview. Can be obtained from list_all_files()[\"files\"][i][\"path\"]. num_lines (int, optional): Number of lines to preview. Defaults to 10. Returns: dict: A dictionary containing: - preview: The first few lines of the file - total_lines: Total number of lines in the file - metadata: File information - error: Error message if reading fails Example: >>> # Get all files first >>> all_files = list_all_files() >>> # Preview first file >>> preview = preview_file(all_files[\"files\"][0][\"path\"], num_lines=5) >>> print(preview[\"preview\"])",
          "parameters": []
        },
        {
          "name": "list_all_files",
          "description": ">>> # Get all files first >>> all_files = list_all_files() >>> # Get info for first file >>> info = file_info(all_files[\"files\"][0][\"path\"]) >>> print(info)",
          "parameters": [
            {
              "name": "info",
              "required": false,
              "type": "string"
            }
          ]
        },
        {
          "name": "find_files_by_type",
          "description": "Find all files of a specific type in a directory and its subdirectories. Args: path (str, optional): The root directory to start from. Defaults to current directory (\".\"). file_type (str, optional): The file extension to search for (e.g., '.py', '.js', '.md'). Returns: dict: A dictionary containing: - files: List of matching files with their details - total_matches: Number of files found - file_type: The type of files searched for Example: >>> result = find_files_by_type(\"/path/to/directory\", file_type=\".py\") >>> print(result[\"files\"])",
          "parameters": []
        }
      ],
      "python_sdk": "import mcp\nfrom mcp.client.streamable_http import streamablehttp_client\nimport json\nimport base64\n\nsmithery_api_key = \"\"\nurl = f\"https://server.smithery.ai/@Raghu6798/browser_scrape_mcp/mcp?api_key={smithery_api_key}\"\n\nasync def main():\n    # Connect to the server using HTTP client\n    async with streamablehttp_client(url) as (read_stream, write_stream, _):\n        async with mcp.ClientSession(read_stream, write_stream) as session:\n            # Initialize the connection\n            await session.initialize()\n            # List available tools\n            tools_result = await session.list_tools()\n            print(f\"Available tools: {', '.join([t.name for t in tools_result.tools])}\")\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())",
      "configuration_schema": "",
      "smithery_configuration_requirements": [
        {
          "name": "MISTRAL_API_KEY",
          "required": true,
          "description": "API key for Mistral AI"
        },
        {
          "name": "FIRECRAWL_API_KEY",
          "required": true,
          "description": "API key for Firecrawl service"
        }
      ],
      "python_sdk_config": "",
      "python_sdk_url": "https://server.smithery.ai/@Raghu6798/browser_scrape_mcp/mcp?api_key={smithery_api_key}"
    },
    "source_filename": "cf_1134.@Raghu6798_browser_scrape_mcp_prepared.json",
    "processed_timestamp": 1753731940,
    "processing_mode": "smithery",
    "rank": 1111
  }
}