{
  "labels": {
    "analysis": "The MCP Server \"LM Studio Bridge\" primarily enables interaction with local Large Language Models (LLMs) hosted in LM Studio, facilitating text generation and chat completions. It acts as a bridge between Claude and local models, offering functionalities like connection checks, model listing, text generation, and chat-based interactions. The server's core purpose is to enhance AI capabilities by seamlessly integrating locally run LLMs into existing workflows.",
    "reasoning": "The \"AI/ML Tools\" label is chosen as the primary category because the server's functionality revolves around interacting with and leveraging machine learning models, particularly LLMs. This aligns with its core purpose of enabling local model access and text generation. \"Development Tools\" is included as a secondary label since the server facilitates API-like interactions with models, which could be useful in development contexts. A custom label is added to highlight the server's specific role in bridging between Claude and local models.",
    "primary_label": "AI/ML Tools",
    "secondary_labels": [
      "Development Tools"
    ],
    "custom_label": "LLM Integration Bridge",
    "is_connected": true,
    "is_remote_tool_valid": false,
    "featured_server": false
  },
  "metadata": {
    "server_id": 1989,
    "server_name": "LM Studio Bridge",
    "rank_by_usage": 1990,
    "usage_count": "Not available",
    "original_file": "../crawler/smithery/@infinitimeless_claude-lmstudio-bridge.json",
    "mode": "smithery",
    "timestamp": 1751938055,
    "remote_server_response": {
      "url": "https://server.smithery.ai/@infinitimeless/claude-lmstudio-bridge/mcp?config=eyJkZWJ1ZyI6IGZhbHNlLCAibG1zdHVkaW9Ib3N0IjogIjEyNy4wLjAuMSIsICJsbXN0dWRpb1BvcnQiOiAxMjM0fQ==&api_key=8675feae-43b6-4170-beb5-d8fa5a938222&profile=monetary-anteater-CCaAaT",
      "is_success": true,
      "error": null,
      "tools": [
        {
          "name": "check_lmstudio_connection",
          "description": "Check if the LM Studio server is running and accessible.\n        \n        Returns:\n            Connection status and model information\n        ",
          "input_schema": {
            "properties": {},
            "title": "check_lmstudio_connectionArguments",
            "type": "object"
          },
          "annotations": null
        },
        {
          "name": "list_lmstudio_models",
          "description": "List available LLM models in LM Studio.\n        \n        Returns:\n            A formatted list of available models with their details.\n        ",
          "input_schema": {
            "properties": {},
            "title": "list_lmstudio_modelsArguments",
            "type": "object"
          },
          "annotations": null
        },
        {
          "name": "generate_text",
          "description": "Generate text using a local LLM in LM Studio.\n        \n        Args:\n            prompt: The text prompt to send to the model\n            model_id: ID of the model to use (leave empty for default model)\n            max_tokens: Maximum number of tokens in the response (default: 1000)\n            temperature: Randomness of the output (0-1, default: 0.7)\n        \n        Returns:\n            The generated text from the local LLM\n        ",
          "input_schema": {
            "properties": {
              "prompt": {
                "title": "Prompt",
                "type": "string"
              },
              "model_id": {
                "default": "",
                "title": "Model Id",
                "type": "string"
              },
              "max_tokens": {
                "default": 1000,
                "title": "Max Tokens",
                "type": "integer"
              },
              "temperature": {
                "default": 0.7,
                "title": "Temperature",
                "type": "number"
              }
            },
            "required": [
              "prompt"
            ],
            "title": "generate_textArguments",
            "type": "object"
          },
          "annotations": null
        },
        {
          "name": "chat_completion",
          "description": "Generate a chat completion using a local LLM in LM Studio.\n        \n        Args:\n            messages: JSON string of messages in the format [{\"role\": \"user\", \"content\": \"Hello\"}, ...]\n              or a simple text string which will be treated as a user message\n            model_id: ID of the model to use (leave empty for default model)\n            max_tokens: Maximum number of tokens in the response (default: 1000)\n            temperature: Randomness of the output (0-1, default: 0.7)\n        \n        Returns:\n            The generated text from the local LLM\n        ",
          "input_schema": {
            "properties": {
              "messages": {
                "title": "Messages",
                "type": "string"
              },
              "model_id": {
                "default": "",
                "title": "Model Id",
                "type": "string"
              },
              "max_tokens": {
                "default": 1000,
                "title": "Max Tokens",
                "type": "integer"
              },
              "temperature": {
                "default": 0.7,
                "title": "Temperature",
                "type": "number"
              }
            },
            "required": [
              "messages"
            ],
            "title": "chat_completionArguments",
            "type": "object"
          },
          "annotations": null
        }
      ],
      "tool_count": 4,
      "tool_names": [
        "check_lmstudio_connection",
        "list_lmstudio_models",
        "generate_text",
        "chat_completion"
      ]
    },
    "server_info_crawled": {
      "id": 1989,
      "name": "LM Studio Bridge",
      "author": "@infinitimeless/claude-lmstudio-bridge",
      "overview": "Connect Claude with your local LLMs in LM Studio to access models, generate text, and perform chat completions seamlessly. Enhance your AI interactions by leveraging the capabilities of your local models with ease.",
      "repository_url": "https://github.com/infinitimeless/claude-lmstudio-bridge",
      "homepage": "https://smithery.ai/server/@infinitimeless/claude-lmstudio-bridge",
      "remote_or_local": "Remote",
      "license": "Smithery",
      "usage_count": "Not available",
      "success_rate": "Not available",
      "tags": [
        "search",
        "web",
        "api",
        "mcp"
      ],
      "categories": [
        "search",
        "api"
      ],
      "file_path": "../crawler/smithery/@infinitimeless_claude-lmstudio-bridge.json",
      "tools_count": 4,
      "tools": [
        {
          "name": "check_lmstudio_connection",
          "description": "Check if the LM Studio server is running and accessible.\n        \n        Returns:\n            Connection status and model information\n        ",
          "input_schema": {
            "properties": {},
            "title": "check_lmstudio_connectionArguments",
            "type": "object"
          },
          "annotations": null
        },
        {
          "name": "list_lmstudio_models",
          "description": "List available LLM models in LM Studio.\n        \n        Returns:\n            A formatted list of available models with their details.\n        ",
          "input_schema": {
            "properties": {},
            "title": "list_lmstudio_modelsArguments",
            "type": "object"
          },
          "annotations": null
        },
        {
          "name": "generate_text",
          "description": "Generate text using a local LLM in LM Studio.\n        \n        Args:\n            prompt: The text prompt to send to the model\n            model_id: ID of the model to use (leave empty for default model)\n            max_tokens: Maximum number of tokens in the response (default: 1000)\n            temperature: Randomness of the output (0-1, default: 0.7)\n        \n        Returns:\n            The generated text from the local LLM\n        ",
          "input_schema": {
            "properties": {
              "prompt": {
                "title": "Prompt",
                "type": "string"
              },
              "model_id": {
                "default": "",
                "title": "Model Id",
                "type": "string"
              },
              "max_tokens": {
                "default": 1000,
                "title": "Max Tokens",
                "type": "integer"
              },
              "temperature": {
                "default": 0.7,
                "title": "Temperature",
                "type": "number"
              }
            },
            "required": [
              "prompt"
            ],
            "title": "generate_textArguments",
            "type": "object"
          },
          "annotations": null
        },
        {
          "name": "chat_completion",
          "description": "Generate a chat completion using a local LLM in LM Studio.\n        \n        Args:\n            messages: JSON string of messages in the format [{\"role\": \"user\", \"content\": \"Hello\"}, ...]\n              or a simple text string which will be treated as a user message\n            model_id: ID of the model to use (leave empty for default model)\n            max_tokens: Maximum number of tokens in the response (default: 1000)\n            temperature: Randomness of the output (0-1, default: 0.7)\n        \n        Returns:\n            The generated text from the local LLM\n        ",
          "input_schema": {
            "properties": {
              "messages": {
                "title": "Messages",
                "type": "string"
              },
              "model_id": {
                "default": "",
                "title": "Model Id",
                "type": "string"
              },
              "max_tokens": {
                "default": 1000,
                "title": "Max Tokens",
                "type": "integer"
              },
              "temperature": {
                "default": 0.7,
                "title": "Temperature",
                "type": "number"
              }
            },
            "required": [
              "messages"
            ],
            "title": "chat_completionArguments",
            "type": "object"
          },
          "annotations": null
        }
      ],
      "python_sdk": "import mcp\nfrom mcp.client.streamable_http import streamablehttp_client\nimport json\nimport base64\n\nconfig = {\n  \"debug\": false,\n  \"lmstudioHost\": \"127.0.0.1\",\n  \"lmstudioPort\": 1234\n}\n# Encode config in base64\nconfig_b64 = base64.b64encode(json.dumps(config).encode()).decode()\nsmithery_api_key = \"\"\n\n# Create server URL\nurl = f\"https://server.smithery.ai/@infinitimeless/claude-lmstudio-bridge/mcp?config={config_b64}&api_key={smithery_api_key}\"\n\nasync def main():\n    # Connect to the server using HTTP client\n    async with streamablehttp_client(url) as (read_stream, write_stream, _):\n        async with mcp.ClientSession(read_stream, write_stream) as session:\n            # Initialize the connection\n            await session.initialize()\n            # List available tools\n            tools_result = await session.list_tools()\n            print(f\"Available tools: {', '.join([t.name for t in tools_result.tools])}\")\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())",
      "configuration_schema": "",
      "smithery_configuration_requirements": [],
      "python_sdk_config": "{\n  \"debug\": false,\n  \"lmstudioHost\": \"127.0.0.1\",\n  \"lmstudioPort\": 1234\n}",
      "python_sdk_url": "https://server.smithery.ai/@infinitimeless/claude-lmstudio-bridge/mcp?config={config_b64}&api_key={smithery_api_key}"
    },
    "source_filename": "1990.@infinitimeless_claude-lmstudio-bridge_prepared.json",
    "processed_timestamp": 1753731940,
    "processing_mode": "smithery",
    "rank": 1736
  }
}